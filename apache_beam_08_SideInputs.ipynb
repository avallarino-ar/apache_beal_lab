{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbLwcf7P98xD"
   },
   "source": [
    "### Side Inputs\n",
    "\n",
    "In addition to the main input PCollection, you can provide additional inputs to a ParDo transform in the form of side inputs. A side input is an additional input that your DoFn can access each time it processes an element in the input PCollection. When you specify a side input, you create a view of some other data that can be read from within the ParDo transform’s DoFn while processing each element.\n",
    "\n",
    "Side inputs are useful if your ParDo needs to inject additional data when processing each element in the input PCollection, but the additional data needs to be determined at runtime (and not hard-coded). Such values might be determined by the input data, or depend on a different branch of your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('503996WI Edouard', 31)\n",
      "('957149WC Kyle', 31)\n",
      "('241316NX Kumiko', 31)\n",
      "('796656IE Gaston', 31)\n",
      "('718737IX Ayumi', 30)\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "side_list=list()\n",
    "# IDs to be excluded:\n",
    "with open ('data/exclude_ids.txt','r') as my_file:\n",
    "    for line in my_file:\n",
    "        side_list.append(line.rstrip())\n",
    "\n",
    "# We can pass side inputs to a ParDo transform, which will get passed to its process method.\n",
    "# The first two arguments for the process method would be self and element.\n",
    "\n",
    "class FilterUsingLength(beam.DoFn):\n",
    "    def process(self, element, side_list, lower_bound, upper_bound=float('inf')):\n",
    "        id = element.split(',')[0]\n",
    "        name = element.split(',')[1]\n",
    "        # id = id.decode('utf-8','ignore').encode(\"utf-8\")\n",
    "        element_list= element.split(',')\n",
    "        if (lower_bound <= len(name) <= upper_bound) and id not in side_list:\n",
    "            yield element_list\n",
    "\n",
    "with beam.Pipeline() as pipe:\n",
    "    \"\"\" using pardo to filter names with length between 3 and 10 and exclude IDs with side inputs \"\"\"\n",
    "    small_names =(\n",
    "        pipe\n",
    "        | \"Read file\" >> beam.io.ReadFromText('data/dept_data.txt')\n",
    "        | \"ParDo with side inputs\" >> beam.ParDo(FilterUsingLength(), side_list, 3, 10) \n",
    "        | beam.Filter(lambda record: record[3] == 'Accounts')\n",
    "        | beam.Map(lambda record: (record[0]+ \" \" + record[1], 1))\n",
    "        | beam.CombinePerKey(sum)\n",
    "        # | 'Write results' >> beam.io.WriteToText('output/output_new_final')\n",
    "        | 'Print results' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "# visualize data\n",
    "# !{('head -n 20 output/output_new_final-00000-of-00001')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itoe\n",
      "Kyle\n",
      "Kyle\n",
      "Olga\n",
      "Kirk\n",
      "Marco\n",
      "Rebekah\n",
      "Edouard\n",
      "Kumiko\n",
      "Gaston\n",
      "Ayumi\n",
      "Ayumi\n",
      "Ayumi\n",
      "Ayumi\n",
      "Ayumi\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "# DoFn function \n",
    "class ProcessWords(beam.DoFn):\n",
    "    def process(self, element, cutoff_length, marker):\n",
    "        name = element.split(',')[1]\n",
    "    \n",
    "        if len(name) <= cutoff_length:\n",
    "            yield beam.pvalue.TaggedOutput('Short_Names', name)\n",
    "        else:\n",
    "            yield beam.pvalue.TaggedOutput('Long_Names', name)\n",
    "        \n",
    "        if name.startswith(marker):\n",
    "            yield name\n",
    "        \n",
    "        \n",
    "with beam.Pipeline() as pipe:\n",
    "    results = (\n",
    "        pipe\n",
    "        | beam.io.ReadFromText('data/dept_data.txt')\n",
    "        | beam.ParDo(ProcessWords(), cutoff_length=4, marker='A').with_outputs('Short_Names', \n",
    "                                                                               'Long_Names', \n",
    "                                                                               main='Names_A')\n",
    "    )\n",
    "\n",
    "short_collection = results.Short_Names\n",
    "long_collection = results.Long_Names\n",
    "startA_collection = results.Names_A  \n",
    "\n",
    "# write to file  \n",
    "short_collection | 'Write 1'>> beam.io.WriteToText('output/short')\n",
    "\n",
    "# write to file\n",
    "long_collection | 'Write 2'>> beam.io.WriteToText('output/long')\n",
    "\n",
    "# write to file\n",
    "startA_collection | 'Write 3'>> beam.io.WriteToText('output/start_a')\n",
    "\n",
    "!{'head -n 5 output/short-00000-of-00001'}\n",
    "!{'head -n 5 output/long-00000-of-00001'}\n",
    "!{'head -n 5 output/start_a-00000-of-00001'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itoe\n",
      "Kyle\n",
      "Kyle\n",
      "Olga\n",
      "Kirk\n",
      "Marco\n",
      "Rebekah\n",
      "Edouard\n",
      "Kumiko\n",
      "Gaston\n",
      "Ayumi\n",
      "Ayumi\n",
      "Ayumi\n",
      "Ayumi\n",
      "Ayumi\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "# DoFn function \n",
    "class ProcessWords(beam.DoFn):\n",
    "    def process(self, element, cutoff_length, marker):\n",
    "        name = element.split(',')[1]\n",
    "\n",
    "        if len(name) <= cutoff_length:\n",
    "            yield beam.pvalue.TaggedOutput('Short_Names', name)\n",
    "        else:\n",
    "            yield beam.pvalue.TaggedOutput('Long_Names', name)\n",
    "        \n",
    "        if name.startswith(marker):\n",
    "            yield name \n",
    "    \n",
    "with beam.Pipeline() as pipe:\n",
    "    # the main tag (if specified) first:\n",
    "    startA_collection, short_collection, long_collection = (\n",
    "        pipe\n",
    "        | beam.io.ReadFromText('data/dept_data.txt')\n",
    "        | beam.ParDo(ProcessWords(), cutoff_length=4, marker='A').with_outputs('Short_Names',\n",
    "                                                                               'Long_Names',\n",
    "                                                                               main='Names_A')\n",
    "    )\n",
    "\n",
    "# write to file  \n",
    "short_collection | 'Write 1'>> beam.io.WriteToText('output/short')\n",
    "\n",
    "# write to file\n",
    "long_collection | 'Write 2'>> beam.io.WriteToText('output/long')\n",
    "\n",
    "# write to file\n",
    "startA_collection | 'Write 3'>> beam.io.WriteToText('output/start_a')\n",
    "\n",
    "!{'head -n 5 output/short-00000-of-00001'}\n",
    "!{'head -n 5 output/long-00000-of-00001'}\n",
    "!{'head -n 5 output/start_a-00000-of-00001'}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "apache_beam_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
